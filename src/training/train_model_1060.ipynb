{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of motokimura's baseline solution for the [Solafune Identifying Deforestation Drivers competition](https://solafune.com/competitions/68ad4759-4686-4bb3-94b8-7063f755b43d?menu=about&tab=overview).\n",
    "See https://github.com/motokimura/solafune_deforestation_baseline for the complete code.\n",
    "\n",
    "> cf. @solafune (https://solafune.com) Use for any purpose other than participation in the competition or commercial use is prohibited. If you would like to use them for any of the above purposes, please contact us.\n",
    "\n",
    "### Description\n",
    "\n",
    "**By running this notebook, you will achieve a score of around 0.533 on the public leaderboard.**\n",
    "\n",
    "This notebook trains a U-Net model for 4-class segmentation (`grassland_shrubland`, `logging`, `mining`, and `plantation`) and generates a submission JSON file for the evaluation images from the output from the U-Net model.\n",
    "\n",
    "The submission JSON file is saved to `data/submission.json`.\n",
    "\n",
    "Before running this notebook, you have to run `generate_masks.ipynb` to generate `.npy` files used for training\n",
    "(`generate_masks.ipynb` is available from https://github.com/motokimura/solafune_deforestation_baseline).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Organize the dataset as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── evaluation_images/\n",
    "│   ├── evaluation_0.tif\n",
    "│   ├── evaluation_1.tif\n",
    "│   ├── evaluation_2.tif\n",
    "│   ├── ...\n",
    "├── train_images/\n",
    "│   ├── train_0.tif\n",
    "│   ├── train_1.tif\n",
    "│   ├── train_2.tif\n",
    "│   ├── ...\n",
    "├── train_masks/\n",
    "│   ├── train_0.npy\n",
    "│   ├── train_1.npy\n",
    "│   ├── train_2.npy\n",
    "│   ├── ...\n",
    "```\n",
    "\n",
    "`evaluation_images` and `train_images` can be downloaded from the competition page.\n",
    "\n",
    "`train_masks` can be generated by running `generate_masks.ipynb` available from https://github.com/motokimura/solafune_deforestation_baseline.\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "Please install the python packages imported the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA devices: 1\n",
      "GPU Name: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as albu  # tested with 1.4.24\n",
    "import imagecodecs\n",
    "import numpy as np  # tested with 1.26.4\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl  # tested with 2.5.0.post0\n",
    "import segmentation_models_pytorch as smp  # tested with 0.3.4\n",
    "import sklearn\n",
    "import tensorboard\n",
    "import tifffile\n",
    "import timm  # tested with 0.9.7\n",
    "import torch  # tested with 2.5.1\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, shape\n",
    "from skimage import measure\n",
    "from timm.optim import create_optimizer_v2\n",
    "from timm.scheduler import create_scheduler_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH: C:\\Users\\Brage\\Desktop\\skole\\solafune-deforestation\\data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Move up from the notebook directory to project root\n",
    "BASE_DIR = Path.cwd().resolve().parent.parent  # Moves up two levels to `solafune-deforestation`\n",
    "sys.path.insert(0, str(BASE_DIR))  # Add project root to Python path\n",
    "\n",
    "# Import paths dynamically\n",
    "from src.paths import DATASET_PATH, TRAIN_IMAGES_PATH, EVAL_IMAGES_PATH, TRAIN_ANNOTATIONS_PATH\n",
    "\n",
    "print(\"DATASET_PATH:\", DATASET_PATH)  # Debugging\n",
    "\n",
    "\n",
    "class_names = [\"grassland_shrubland\", \"logging\", \"mining\", \"plantation\"]\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class to load images and masks for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = np.load(mask_path)  # (4, H, W), uint8\n",
    "    assert mask.shape == (4, 1024, 1024)\n",
    "    mask = mask.transpose(1, 2, 0)  # (H, W, 4)\n",
    "    return mask.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tifffile.imread(image_path)  # (H, W, 12), float64\n",
    "    assert image.shape == (1024, 1024, 12)\n",
    "    image = np.nan_to_num(image)  # replace NaN with 0\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # mean of train images\n",
    "    mean = np.array(\n",
    "        [\n",
    "            285.8190561180765,\n",
    "            327.22091430696577,\n",
    "            552.9305957826701,\n",
    "            392.1575148484924,\n",
    "            914.3138803812591,\n",
    "            2346.1184507500043,\n",
    "            2884.4831706095824,\n",
    "            2886.442429854111,\n",
    "            3176.7501338557763,\n",
    "            3156.934442092072,\n",
    "            1727.1940075511282,\n",
    "            848.573373995044,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # std of train images\n",
    "    std = np.array(\n",
    "        [\n",
    "            216.44975668759372,\n",
    "            269.8880248304874,\n",
    "            309.92790753407064,\n",
    "            397.45655590699,\n",
    "            400.22078920482215,\n",
    "            630.3269651264278,\n",
    "            789.8006920468097,\n",
    "            810.4773696969773,\n",
    "            852.9031432100967,\n",
    "            807.5976198303886,\n",
    "            631.7808113929271,\n",
    "            502.66788721341396,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    mean = mean.reshape(12, 1, 1)\n",
    "    std = std.reshape(12, 1, 1)\n",
    "\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "class TrainValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, sample_indices, augmentations=None):\n",
    "        self.image_paths, self.mask_paths = [], []\n",
    "        for i in sample_indices:\n",
    "            self.image_paths.append(data_root / \"train_images\" / f\"train_{i}.tif\")\n",
    "            self.mask_paths.append(data_root / \"train_masks\" / f\"train_{i}.npy\")\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "            \"mask\": load_mask(self.mask_paths[idx]),\n",
    "        }\n",
    "\n",
    "        if self.augmentations is not None:\n",
    "            sample = self.augmentations(**sample)\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"mask\"] = sample[\"mask\"].transpose(2, 0, 1)  # (4, H, W)\n",
    "\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "        sample[\"mask_path\"] = str(self.mask_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define U-Net model using pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # prepare segmentation model\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"unet\",\n",
    "            encoder_name=\"tu-tf_efficientnetv2_s\",  # use `tf_efficientnetv2_s` from timm\n",
    "            encoder_weights=\"imagenet\",  # always starts from imagenet pre-trained weight\n",
    "            in_channels=12,\n",
    "            classes=4,\n",
    "        )\n",
    "\n",
    "        # prepare loss functions\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.0)\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # assuming image is already normalized\n",
    "        return self.model(image)  # logits\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        loss = self.dice_loss_fn(logits_mask, mask) + self.bce_loss_fn(logits_mask, mask)\n",
    "\n",
    "        # count tp, fp, fn, tn for each class to compute validation metrics at the end of epoch\n",
    "        thresh = 0.5\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            (prob_mask > thresh).long(),\n",
    "            mask.long(),\n",
    "            mode=smp.losses.MULTILABEL_MODE,\n",
    "        )  # each of tp, fp, fn, tn is a tensor of shape (batch_size, num_classes) and of type long\n",
    "\n",
    "        output = {\n",
    "            \"loss\": loss.detach().cpu(),\n",
    "            \"tp\": tp.detach().cpu(),\n",
    "            \"fp\": fp.detach().cpu(),\n",
    "            \"fn\": fn.detach().cpu(),\n",
    "            \"tn\": tn.detach().cpu(),\n",
    "        }\n",
    "        if stage == \"train\":\n",
    "            self.training_step_outputs.append(output)\n",
    "        else:\n",
    "            self.validation_step_outputs.append(output)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"val\")\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        def log(name, tensor, prog_bar=False):\n",
    "            self.log(f\"{stage}/{name}\", tensor.to(self.device), sync_dist=True, prog_bar=prog_bar)\n",
    "\n",
    "        # aggregate loss\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        # aggregate tp, fp, fn, tn to compose F1 score for each class\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        f1_scores = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            f1_scores[class_name] = smp.metrics.f1_score(tp[:, i], fp[:, i], fn[:, i], tn[:, i], reduction=\"macro-imagewise\")\n",
    "            log(f\"f1/{class_name}\", f1_scores[class_name], prog_bar=False)\n",
    "\n",
    "        f1_avg = torch.stack([v for v in f1_scores.values()]).mean()\n",
    "        log(\"f1\", f1_avg, prog_bar=True)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"val\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=\"adamw\",\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-2,\n",
    "            filter_bias_and_bn=True,  # filter out bias and batchnorm from weight decay\n",
    "        )\n",
    "\n",
    "        # lr scheduler\n",
    "        scheduler, _ = create_scheduler_v2(\n",
    "            optimizer,\n",
    "            sched=\"cosine\",\n",
    "            num_epochs=epochs,\n",
    "            min_lr=0.0,\n",
    "            warmup_lr=1e-5,\n",
    "            warmup_epochs=0,\n",
    "            warmup_prefix=False,\n",
    "            step_on_epochs=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, metric):\n",
    "        # workaround for timm's scheduler:\n",
    "        # https://github.com/Lightning-AI/lightning/issues/5555#issuecomment-1065894281\n",
    "        scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trainer of pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "train_output_dir = DATASET_PATH / \"training_result\"\n",
    "\n",
    "# split train_images into train-set and val-set\n",
    "sample_indices = list(range(176))  # train_0.tif to train_175.tif\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(sample_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# augmentations applied only to train-set\n",
    "augmentations = albu.Compose(\n",
    "[        \n",
    "        # shift, scale, and rotate\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=0,  # constant border\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            interpolation=2,  # bicubic\n",
    "        ),\n",
    "        # random crop\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        # flip, transpose, and rotate90\n",
    "        albu.Resize(512, 512),  # Resize images\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prepare data loaders\n",
    "num_workers = min(2, os.cpu_count())  # Adaptive worker selection\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        DATASET_PATH,\n",
    "        train_indices,\n",
    "        augmentations=augmentations,\n",
    "    ),\n",
    "    batch_size=4,  # Reduce batch size\n",
    "    num_workers=0,  # Reduce workers to avoid memory overhead\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        DATASET_PATH,\n",
    "        val_indices,\n",
    "        augmentations=None,\n",
    "    ),\n",
    "    batch_size=2,  # Reduce validation batch size\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation F1 score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_f1_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=5,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0,],\n",
    "    strategy=\"auto\",\n",
    "    log_every_n_steps=5,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "\n",
    "# prepare model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!\n",
    "\n",
    "With the default setting, 10 GB of GPU memory is required. To reduce the memory usage, you can decrease the batch size.\n",
    "\n",
    "The trained model is saved as `data/training_result/best_f1_05.ckpt`.\n",
    "\n",
    "Tensorboard logs are also saved under `data/training_result/version_xx`.\n",
    "\n",
    "**The execution often does not finish even after reaching 200 epochs. In that case, you can stop the execution manually and just proceed to the next cell (do not restart the notebook!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                  | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model        | Unet                  | 22.1 M | train\n",
      "1 | dice_loss_fn | DiceLoss              | 0      | train\n",
      "2 | bce_loss_fn  | SoftBCEWithLogitsLoss | 0      | train\n",
      "---------------------------------------------------------------\n",
      "22.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.383    Total estimated model params size (MB)\n",
      "759       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50abe7489cf428f8dbc9f8ac6edf1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c382c70f31447a915bb0b79278ef4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9349d4b8095341f4916771c3602e2571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24da66c025e41b086c5d8431dd11bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add0097034574cb09c2a68edf08544a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e5c0d5620943ea818a64279b46ebc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed22c03e147434b90da277a08675eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bf074132ba41a6a6ed745661d33aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f451300731e4dc2b56794b65f64c8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4ad8c8b24247098b091ee099de8fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7487d8bd11954d8496354c77e19e0e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a6a07de46d49aa834749bac46eebe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e358ebc699b64081aee83828615177e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a61a226ba343238e0a727b7411f9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d655d60f81e54ab093b046eb9f15b9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4af839f4844dcca04f3a35f4a40232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fec3c7a4a94c90885859ae724df045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71437833179a4bedb717df16809eeda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b050549b114a36a2381bf0e5b1da22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd75f7b784646e6b51812a0b98050dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957c0d2258d84a3588a036c666ef14ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da48e4c9ccf4aae99c52ac6aaed9649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c1fa5380ad4f1e97f2f1b3a9ab0876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71179fee80a43b68b5c2f70b7ff283a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a39429274746369610b04f79a8bef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31c880bd36d45e98d1c4ab23293a668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4f9dcd44c84fe9b5aa4163d817b3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60589a2d06d24ee4ac9ac9cbb518d7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2750e69d2964dc69c55521701612d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc750b0fa8e04807a499eb5025cfac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446880a1ff82419b88f35a22625a862a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b1b5e0c31b4a4694d9c7db9dc119b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dafee6bfb44beca9f6352e49a689a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0192a60842d4e0b81b34f3b5eb7b619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7981ec8972480180dd05f37704e40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c0aa7c24b47d2b043e0ddead6c61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adbea494d7f4cf4bddd841fc5342839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e962a88de564d7987d61ee85f2a55e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae9b4f33d0d4e2886a3f814deb7b22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224425573dd9430d98fe7ee8cad66c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96614138a4a4fd3ac5f30cd894f0681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d308f50843624597a089308b21493040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the evaluation metric for the validation set\n",
    "\n",
    "Before predicting the evaluation images, let's predict the validation set and compute the evaluation metric.\n",
    "\n",
    "This may be useful to check if the model is trained properly and to tune the parameters for post-processing (e.g., score threshold, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, loader, pred_output_dir):\n",
    "    pred_output_dir = Path(pred_output_dir)\n",
    "    pred_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        img = batch[\"image\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_mask = model(img)\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "\n",
    "        # save prob mask as numpy array\n",
    "        for i in range(img.size(0)):\n",
    "            file_name = os.path.basename(batch[\"image_path\"][i])\n",
    "            prob_mask_i = prob_mask[i].cpu().numpy()  # (4, 1024, 1024)\n",
    "\n",
    "            np.save(\n",
    "                pred_output_dir / file_name.replace(\".tif\", \".npy\"),\n",
    "                prob_mask_i.astype(np.float16),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:23<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint and run inference on val-set\n",
    "del model\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_f1_05-v2.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = DATASET_PATH / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val f1 score: 0.6974\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grassland_shrubland</th>\n",
       "      <th>logging</th>\n",
       "      <th>mining</th>\n",
       "      <th>plantation</th>\n",
       "      <th>all_classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_9</th>\n",
       "      <td>0.943992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606809</td>\n",
       "      <td>0.651702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_16</th>\n",
       "      <td>0.438303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>0.843038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_18</th>\n",
       "      <td>0.593364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956783</td>\n",
       "      <td>0.887537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.549777</td>\n",
       "      <td>0.637444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910089</td>\n",
       "      <td>0.977522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.523292</td>\n",
       "      <td>0.630823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_30</th>\n",
       "      <td>0.988026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_31</th>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910657</td>\n",
       "      <td>0.727664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653874</td>\n",
       "      <td>0.663468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_45</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190453</td>\n",
       "      <td>0.547613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_55</th>\n",
       "      <td>0.870470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_60</th>\n",
       "      <td>0.669045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_65</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807772</td>\n",
       "      <td>0.943392</td>\n",
       "      <td>0.687791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_66</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.477491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_67</th>\n",
       "      <td>0.210542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_82</th>\n",
       "      <td>0.166531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.716419</td>\n",
       "      <td>0.507311</td>\n",
       "      <td>0.597565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443919</td>\n",
       "      <td>0.360980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_109</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873940</td>\n",
       "      <td>0.968485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_111</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682926</td>\n",
       "      <td>0.670732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_113</th>\n",
       "      <td>0.050581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756095</td>\n",
       "      <td>0.451669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_114</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985409</td>\n",
       "      <td>0.746352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_117</th>\n",
       "      <td>0.368508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962844</td>\n",
       "      <td>0.832838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_118</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891452</td>\n",
       "      <td>0.722863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_119</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780350</td>\n",
       "      <td>0.695087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_127</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_139</th>\n",
       "      <td>0.560034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.686289</td>\n",
       "      <td>0.811581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_140</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963084</td>\n",
       "      <td>0.490771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_144</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.665075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_150</th>\n",
       "      <td>0.633934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804915</td>\n",
       "      <td>0.859712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_160</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_images</th>\n",
       "      <td>0.447284</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.892879</td>\n",
       "      <td>0.727161</td>\n",
       "      <td>0.697387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            grassland_shrubland   logging    mining  plantation  all_classes\n",
       "train_9                0.943992  1.000000  1.000000    1.000000     0.985998\n",
       "train_12               1.000000  1.000000  1.000000    0.000000     0.750000\n",
       "train_15               0.000000  1.000000  1.000000    0.606809     0.651702\n",
       "train_16               0.438303  1.000000  1.000000    0.933849     0.843038\n",
       "train_18               0.593364  1.000000  1.000000    0.956783     0.887537\n",
       "train_19               0.000000  1.000000  1.000000    0.549777     0.637444\n",
       "train_24               1.000000  1.000000  1.000000    0.910089     0.977522\n",
       "train_29               0.000000  1.000000  1.000000    0.523292     0.630823\n",
       "train_30               0.988026  1.000000  1.000000    1.000000     0.997006\n",
       "train_31               0.608888  0.000000  0.000000    0.000000     0.152222\n",
       "train_41               0.000000  1.000000  1.000000    0.910657     0.727664\n",
       "train_42               0.000000  1.000000  1.000000    0.653874     0.663468\n",
       "train_45               1.000000  1.000000  0.000000    0.190453     0.547613\n",
       "train_55               0.870470  1.000000  1.000000    1.000000     0.967618\n",
       "train_60               0.669045  1.000000  0.912860    0.000000     0.645476\n",
       "train_65               0.000000  1.000000  0.807772    0.943392     0.687791\n",
       "train_66               0.000000  0.000000  0.909963    1.000000     0.477491\n",
       "train_67               0.210542  1.000000  0.796633    1.000000     0.751794\n",
       "train_82               0.166531  1.000000  0.716419    0.507311     0.597565\n",
       "train_90               0.000000  1.000000  0.000000    0.443919     0.360980\n",
       "train_109              1.000000  1.000000  1.000000    0.873940     0.968485\n",
       "train_111              1.000000  0.000000  1.000000    0.682926     0.670732\n",
       "train_113              0.050581  0.000000  1.000000    0.756095     0.451669\n",
       "train_114              1.000000  0.000000  1.000000    0.985409     0.746352\n",
       "train_117              0.368508  1.000000  1.000000    0.962844     0.832838\n",
       "train_118              0.000000  1.000000  1.000000    0.891452     0.722863\n",
       "train_119              0.000000  1.000000  1.000000    0.780350     0.695087\n",
       "train_127              0.000000  1.000000  1.000000    1.000000     0.750000\n",
       "train_139              0.560034  1.000000  1.000000    0.686289     0.811581\n",
       "train_140              0.000000  0.000000  1.000000    0.963084     0.490771\n",
       "train_144              0.000000  1.000000  1.000000    0.660300     0.665075\n",
       "train_150              0.633934  1.000000  1.000000    0.804915     0.859712\n",
       "train_160              0.000000  0.000000  1.000000    1.000000     0.500000\n",
       "train_167              1.000000  0.000000  1.000000    0.000000     0.500000\n",
       "train_169              1.000000  0.000000  1.000000    1.000000     0.750000\n",
       "train_172              1.000000  0.000000  1.000000    1.000000     0.750000\n",
       "all_images             0.447284  0.722222  0.892879    0.727161     0.697387"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_f1_score(pred_mask, truth_mask):\n",
    "    # `pred_mask` is a binary numpy array of shape (H, W) = (1024, 1024)\n",
    "    # `truth_mask` is a binaru numpy array of shape (H, W) = (1024, 1024)\n",
    "    assert pred_mask.shape == (1024, 1024), f\"{pred_mask.shape=}\"\n",
    "    assert truth_mask.shape == (1024, 1024), f\"{truth_mask.shape=}\"\n",
    "\n",
    "    tp = ((pred_mask > 0) & (truth_mask > 0)).sum()\n",
    "    fp = ((pred_mask > 0) & (truth_mask == 0)).sum()\n",
    "    fn = ((pred_mask == 0) & (truth_mask > 0)).sum()\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 1  # if no prediction, precision is considered as 1\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 1  # if no ground truth, recall is considered as 1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0  # if either precision or recall is 0, f1 is 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "score_thresh = 0.7  # threshold to binarize the prediction mask\n",
    "min_area = 10000  # if the predicted area of a class is less than this, submit an zero mask because small predicted areas are often false positives\n",
    "\n",
    "val_f1_scores = {}\n",
    "for idx in sorted(val_indices):\n",
    "    fn = f\"train_{idx}\"\n",
    "    # prepare prediction mask\n",
    "    pred_mask = np.load(val_pred_dir / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    pred_mask = pred_mask > score_thresh  # binarize\n",
    "    # prepare ground truth mask\n",
    "    truth_mask = np.load(DATASET_PATH / \"train_masks\" / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    # compute f1 score for each class\n",
    "    val_f1_scores[fn] = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        pred_for_a_class = pred_mask[i]\n",
    "        if pred_for_a_class.sum() < min_area:\n",
    "            pred_for_a_class = np.zeros_like(pred_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "        val_f1_scores[fn][class_name] = compute_f1_score(pred_for_a_class, truth_mask[i])\n",
    "val_f1_scores = pd.DataFrame(val_f1_scores).T\n",
    "\n",
    "# add a column for average of all the 4 classes\n",
    "val_f1_scores[\"all_classes\"] = val_f1_scores.mean(axis=1)\n",
    "# add a row for average of all the val images\n",
    "val_f1_scores.loc[\"all_images\"] = val_f1_scores.mean()\n",
    "\n",
    "print(f\"val f1 score: {val_f1_scores.loc['all_images', 'all_classes']:.4f}\")\n",
    "\n",
    "val_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the evaluation images and generate a submission JSON file\n",
    "\n",
    "Let's predict the evaluation images as already done with the validation set, and generate a submission JSON file.\n",
    "\n",
    "The submission JSON file will be saved as `data/submission.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root):\n",
    "        self.image_paths = []\n",
    "        for i in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "            self.image_paths.append(data_root / \"evaluation_images\" / f\"evaluation_{i}.tif\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "        }\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:17<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    TestDataset(DATASET_PATH),\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_pred_dir = DATASET_PATH / \"test_preds\"\n",
    "run_inference(model, test_loader, test_pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detect_polygons()` below extracts isolated areas as polygons from the predicted mask.\n",
    "\n",
    "The point is `min_area` parameter to filter out small areas. Small predicted areas are often false positives which decrease the evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polygons(pred_dir, score_thresh, min_area):\n",
    "    pred_dir = Path(pred_dir)\n",
    "    pred_paths = list(pred_dir.glob(\"*.npy\"))\n",
    "    pred_paths = sorted(pred_paths)\n",
    "\n",
    "    polygons_all_imgs = {}\n",
    "    for pred_path in tqdm(pred_paths):\n",
    "        polygons_all_classes = {}\n",
    "\n",
    "        mask = np.load(pred_path)  # (4, 1024, 1024)\n",
    "        mask = mask > score_thresh  # binarize\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            mask_for_a_class = mask[i]\n",
    "            if mask_for_a_class.sum() < min_area:\n",
    "                mask_for_a_class = np.zeros_like(mask_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "\n",
    "            # extract polygons from the binarized mask\n",
    "            label = measure.label(mask_for_a_class, connectivity=2, background=0).astype(np.uint8)\n",
    "            polygons = []\n",
    "            for p, value in features.shapes(label, label):\n",
    "                p = shape(p).buffer(0.5)\n",
    "                p = p.simplify(tolerance=5.0)\n",
    "                polygons.append(p)\n",
    "            polygons_all_classes[class_name] = polygons\n",
    "        polygons_all_imgs[pred_path.name.replace(\".npy\", \".tif\")] = polygons_all_classes\n",
    "\n",
    "    return polygons_all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:47<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_polygons = detect_polygons(test_pred_dir, score_thresh=score_thresh, min_area=min_area)\n",
    "\n",
    "submission_save_path = DATASET_PATH / f\"submission.json\"\n",
    "\n",
    "images = []\n",
    "for img_id in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "    annotations = []\n",
    "    for class_name in class_names:\n",
    "        for poly in test_pred_polygons[f\"evaluation_{img_id}.tif\"][class_name]:\n",
    "            seg: list[float] = []  # [x0, y0, x1, y1, ..., xN, yN]\n",
    "            for xy in poly.exterior.coords:\n",
    "                seg.extend(xy)\n",
    "\n",
    "            annotations.append({\"class\": class_name, \"segmentation\": seg})\n",
    "\n",
    "    images.append({\"file_name\": f\"evaluation_{img_id}.tif\", \"annotations\": annotations})\n",
    "\n",
    "with open(submission_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"images\": images}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (segmentation-rtx1060)",
   "language": "python",
   "name": "segmentation-rtx1060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
